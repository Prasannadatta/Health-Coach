{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import torch\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        ")\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# ========= CONFIG =========\n",
        "MODEL_ID = \"google/gemma-3-270m-it\"\n",
        "\n",
        "DATA_FILES = [\n",
        "    \"/content/exercise_q&a.jsonl\",\n",
        "]\n",
        "\n",
        "VAL_FRACTION = 0.1\n",
        "\n",
        "\n",
        "# ========= DATA LOADING =========\n",
        "def read_jsonl(path):\n",
        "    rows = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "\n",
        "def read_json(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    if isinstance(data, list):\n",
        "        return data\n",
        "    elif isinstance(data, dict):\n",
        "        for v in data.values():\n",
        "            if isinstance(v, list) and v and isinstance(v[0], dict):\n",
        "                return v\n",
        "    raise ValueError(f\"Don't know how to interpret JSON structure in {path}\")\n",
        "\n",
        "\n",
        "def load_all_examples():\n",
        "    all_rows = []\n",
        "    for path in DATA_FILES:\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"Warning: {path} does not exist, skipping.\")\n",
        "            continue\n",
        "\n",
        "        ext = os.path.splitext(path)[1].lower()\n",
        "        try:\n",
        "            if ext == \".jsonl\":\n",
        "                rows = read_jsonl(path)\n",
        "            elif ext == \".json\":\n",
        "                rows = read_json(path)\n",
        "            else:\n",
        "                print(f\"Warning: unsupported extension {ext} for {path}, skipping.\")\n",
        "                continue\n",
        "\n",
        "            for r in rows:\n",
        "                if \"question\" in r and \"answer\" in r:\n",
        "                    all_rows.append({\"question\": r[\"question\"], \"answer\": r[\"answer\"]})\n",
        "                elif \"input\" in r and \"output\" in r:\n",
        "                    all_rows.append({\"question\": r[\"input\"], \"answer\": r[\"output\"]})\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: failed to read {path}: {e}\")\n",
        "\n",
        "    if not all_rows:\n",
        "        raise RuntimeError(\"No examples loaded. Check DATA_FILES paths & format.\")\n",
        "\n",
        "    return all_rows\n",
        "\n",
        "\n",
        "def make_dataset():\n",
        "    rows = load_all_examples()\n",
        "    random.shuffle(rows)\n",
        "\n",
        "    n_total = len(rows)\n",
        "    n_val = max(1, int(VAL_FRACTION * n_total))\n",
        "\n",
        "    return DatasetDict({\n",
        "        \"train\": Dataset.from_list(rows[n_val:]),\n",
        "        \"val\": Dataset.from_list(rows[:n_val])\n",
        "    })\n",
        "\n",
        "\n",
        "# ========= TOKENIZER & MODEL =========\n",
        "def make_tokenizer():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def make_model(device: str):\n",
        "    dtype = torch.bfloat16 if device == \"mps\" else torch.float32\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        dtype=dtype,\n",
        "    )\n",
        "    model.to(device)\n",
        "    return model\n",
        "\n",
        "\n",
        "# ========= PROMPT FORMATTING =========\n",
        "def format_example(example):\n",
        "    question = example[\"question\"]\n",
        "    answer = example[\"answer\"]\n",
        "\n",
        "    text = (\n",
        "        f\"Question: {question}\\n\\n\"\n",
        "        f\"Answer: {answer}\"\n",
        "    )\n",
        "    return text\n",
        "\n",
        "\n",
        "def formatting_func(example):\n",
        "    return format_example(example)\n",
        "\n",
        "\n",
        "# ========= MAIN =========\n",
        "def main():\n",
        "    if torch.backends.mps.is_available():\n",
        "        device = \"mps\"\n",
        "    elif torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    dataset = make_dataset()\n",
        "    tokenizer = make_tokenizer()\n",
        "    model = make_model(device)\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\n",
        "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "            \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    sft_config = SFTConfig(\n",
        "        output_dir=\"lora-gemma3-270m-it\",\n",
        "        do_train=True,\n",
        "        do_eval=True,\n",
        "        per_device_train_batch_size=1,\n",
        "        per_device_eval_batch_size=1,\n",
        "        gradient_accumulation_steps=8,\n",
        "        num_train_epochs=2.0,\n",
        "        learning_rate=2e-4,\n",
        "        weight_decay=0.01,\n",
        "        warmup_ratio=0.03,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        logging_steps=10,\n",
        "        save_strategy=\"epoch\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_total_limit=2,\n",
        "        report_to=None,\n",
        "        max_length=512,\n",
        "        packing=False,\n",
        "        use_mps_device=(device == \"mps\"),\n",
        "        fp16=False,\n",
        "        bf16=False,\n",
        "        remove_unused_columns=False,\n",
        "    )\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        args=sft_config,\n",
        "        peft_config=lora_config,\n",
        "        train_dataset=dataset[\"train\"],\n",
        "        eval_dataset=dataset[\"val\"],\n",
        "        formatting_func=formatting_func,\n",
        "        processing_class=tokenizer,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    save_dir = \"lora-gemma3-270m-it-adapter\"\n",
        "    trainer.model.save_pretrained(save_dir)\n",
        "    tokenizer.save_pretrained(save_dir)\n",
        "    print(f\"Saved LoRA adapter to {save_dir}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Lvc2r92aCutQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "\n",
        "lora_path = \"lora-gemma3-270m-it-adapter\"\n",
        "MODEL_ID = \"google/gemma-3-270m-it\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(MODEL_ID)\n",
        "base_model.to(device)\n",
        "\n",
        "# Load merged LoRA model\n",
        "model = PeftModel.from_pretrained(base_model, lora_path)\n",
        "model.to(device)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "J-5LT6giCv50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = (\n",
        "    \"question: Can you recommend me a full body  HIIT workout?\\n\"\n",
        "    \"answer:\"\n",
        ")\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "output = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=200,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "QFMCY9pnC04R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}